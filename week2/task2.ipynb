{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = '/kaggle/input/blur-dataset/motion_blurred'\n",
    "Output = '/kaggle/input/blur-dataset/sharp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(path):\n",
    "    result = []\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if any(extension in file for extension in ['.jpg', '.png', '.jpeg','JPG']):\n",
    "            img = cv2.imread(os.path.join(path, file))\n",
    "            if img is not None:\n",
    "              img = cv2.resize(img, (128, 128))\n",
    "              result.append(img)\n",
    "            else:\n",
    "              return\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_load(Input)\n",
    "y = data_load(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=35\n",
    "learning_rate=0.001\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms=transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def apply_transforms(images_array,transform):\n",
    "  transformed_images=[]\n",
    "  for img in images_array:\n",
    "    img=Image.fromarray(img)\n",
    "    transformed_img=transform(img)\n",
    "    transformed_images.append(transformed_img)\n",
    "\n",
    "  transformed_images=torch.stack(transformed_images)\n",
    "  return transformed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=apply_transforms(X,all_transforms)\n",
    "y=apply_transforms(y,all_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1=int(len(X)*0.5)\n",
    "ind2=int(len(X)*0.625)\n",
    "\n",
    "X_train=X[:ind1]\n",
    "y_train=y[:ind1]\n",
    "X_val=X[ind1:ind2]\n",
    "y_val=X[ind1:ind2]\n",
    "X_test=X[ind2:]\n",
    "y_test=y[ind2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_block(nn.Module):\n",
    "  def __init__(self,in_channels,out_channels):\n",
    "    super(Encoder_block,self).__init__()\n",
    "    self.conv_layer1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=(1,1))\n",
    "    self.conv_layer2=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,padding=(1,1))\n",
    "    self.max_pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    out=self.relu(self.conv_layer1(x))\n",
    "    out=self.relu(self.conv_layer2(out))\n",
    "    skip=out\n",
    "    out=self.max_pool(out)\n",
    "    return out,skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_block(nn.Module):\n",
    "  def __init__(self,in_channels,out_channels):\n",
    "    super(Decoder_block,self).__init__()\n",
    "    self.upconv_layer=nn.ConvTranspose2d(in_channels=in_channels,out_channels=out_channels,kernel_size=2,stride=2)\n",
    "    self.conv_layer1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=(1,1))\n",
    "    self.conv_layer2=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,padding=(1,1))\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,x,skip):\n",
    "    out=self.upconv_layer(x)\n",
    "    if out.size() != skip.size():\n",
    "         out = F.interpolate(out, size=(skip.size(2), skip.size(3)), mode='bilinear', align_corners=True)\n",
    "            \n",
    "    out=torch.cat((out,skip),dim=1)\n",
    "    out=self.relu(self.conv_layer1(out))\n",
    "    out=self.relu(self.conv_layer2(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "  def __init__(self,in_channels,out_channels):\n",
    "    super(UNet,self).__init__()\n",
    "    self.enc1=Encoder_block(in_channels,64)\n",
    "    self.enc2=Encoder_block(64,128)\n",
    "    self.enc3=Encoder_block(128,256)\n",
    "    self.enc4=Encoder_block(256,512)\n",
    "\n",
    "    self.middle_conv1=nn.Conv2d(512,1024,kernel_size=3,padding=(1,1))\n",
    "    self.middle_conv2=nn.Conv2d(1024,1024,kernel_size=3,padding=(1,1))\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "    self.dec1=Decoder_block(1024,512)\n",
    "    self.dec2=Decoder_block(512,256)\n",
    "    self.dec3=Decoder_block(256,128)\n",
    "    self.dec4=Decoder_block(128,64)\n",
    "\n",
    "    self.final=nn.Conv2d(64,out_channels,kernel_size=1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x1,skip1=self.enc1(x)\n",
    "    x2,skip2=self.enc2(x1)\n",
    "    x3,skip3=self.enc3(x2)\n",
    "    x4,skip4=self.enc4(x3)\n",
    "\n",
    "    x5=self.relu(self.middle_conv1(x4))\n",
    "    x6=self.relu(self.middle_conv2(x5))\n",
    "\n",
    "    x7=self.dec1(x6,skip4)\n",
    "    x8=self.dec2(x7,skip3)\n",
    "    x9=self.dec3(x8,skip2)\n",
    "    x10=self.dec4(x9,skip1)\n",
    "\n",
    "    out=self.final(x10)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=UNet(in_channels=3,out_channels=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "    #Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        print(f\"Before squeeze - outputs shape: {outputs.shape}, labels shape: {labels.shape}\")\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = X_train[1]\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128))  # Resize to match model's input size\n",
    "])\n",
    "input_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_image)\n",
    "\n",
    "outputs=outputs.squeeze()\n",
    "outputs=outputs.permute(1,2,0)\n",
    "output_image = outputs.squeeze().cpu().numpy()   \n",
    "\n",
    "# Display the input image and segmented output\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.permute(1, 2, 0))  \n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_image)\n",
    "plt.title('Output')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
